{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traning for SY 1.9 & 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-30T20:10:43.568069Z",
     "start_time": "2019-12-30T20:10:42.806238Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import os\n",
    "import datetime\n",
    "import glob\n",
    "import csv\n",
    "\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, TensorDataset, ConcatDataset, random_split\n",
    "from torchvision import transforms\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from numpy import genfromtxt\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-30T20:10:43.578542Z",
     "start_time": "2019-12-30T20:10:43.569958Z"
    }
   },
   "outputs": [],
   "source": [
    "def plotSpec(seyfert,n):\n",
    "    if seyfert == 1.9:\n",
    "        plt.plot(np.linspace(3800,7500,7500-3800+1), sy19_feature[n].tolist())\n",
    "    if seyfert == 2:\n",
    "        plt.plot(np.linspace(3800,7500,7500-3800+1), sy2_feature[n].tolist())\n",
    "        \n",
    "def plotData(data):\n",
    "        plt.plot(np.linspace(3800,len(data)-1+3800,len(data)), data.tolist(),linewidth=1)\n",
    "        \n",
    "def plotFilters(filters):\n",
    "    filters = filters.flatten()[1:-1].reshape(-1,2)\n",
    "    for x in filters:\n",
    "        plt.axvspan(x[0]+3800, x[1]+3800, alpha=0.5, facecolor='grey')\n",
    "        \n",
    "def sizeMap(inputSize):\n",
    "    IN = inputSize\n",
    "    C1 = (inputSize-50)+1\n",
    "    S2 = int(C1/2)\n",
    "    C3 = (S2-50)+1\n",
    "    S4 = int(C3/2)\n",
    "    C5 = (S4-50)+1\n",
    "    S6 = int(C5/2)\n",
    "    return IN, C1, S2, C3, S4, C5, S6 \n",
    "\n",
    "\n",
    "def read_data(filename, delimiter=','):\n",
    "    with open(filename, 'r') as my_file:\n",
    "        reader = csv.reader(my_file, delimiter=delimiter)\n",
    "        my_list = list(reader)[0]\n",
    "        my_list= [float(i) for i in my_list]\n",
    "        return my_list\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AGN Emission Lines (3000 - 8000 A)\n",
    "https://ned.ipac.caltech.edu/level5/Netzer/Netzer2_1.html\n",
    "\n",
    "```\n",
    "CIII λ977\n",
    "OVI λ1035\n",
    "Lα\n",
    "NV λ1240\n",
    "OI λ1304\n",
    "CII λ1336\n",
    "SiIV,OIV] λ1400\n",
    "NIV] λ1486\n",
    "CIV λ1549 \n",
    "HeII λ1640\n",
    "OIII] λ1663\n",
    "NIII] λ1750\n",
    "CIII] λ1909\n",
    "FeII λ(2200-2800)  \n",
    "MgII λ2798\n",
    "[NeV] λ3426\n",
    "[OII] λ3727\n",
    "[NeIII] λ3869\n",
    "HeII λ4686\n",
    "Hβ λ4861\n",
    "[OIII] λ4959,5007\n",
    "FeII λ(4500-5400) \n",
    "HeI λ5876\n",
    "[FeVII] λ6087\n",
    "[OI] λ6300\n",
    "[FeX] λ6374\n",
    "Hα λ6563\n",
    "[NII] λ6583\n",
    "[SII] λ6716,6731\n",
    "CaII λ8498-8662\n",
    "[SIII] λ9069,9532\n",
    "```\n",
    "\n",
    "The emission line spectrum of active galactic nuclei and the unifying scheme\n",
    "https://link.springer.com/article/10.1007%2Fs001590000006\n",
    "\n",
    "```\n",
    "(Broad permmited lines)\n",
    "HI, HeI λλ5876,6678,7065\n",
    "He II λ4686\n",
    "Fe II visible domain\n",
    "\n",
    "(narrow line)\n",
    "[Fe VII] λ6087\n",
    "[Fe X] λ6375\n",
    "[Fe XI] λ7892\n",
    "```\n",
    "\n",
    "add more balmer lines\n",
    "```\n",
    "Hγ λ4340\n",
    "H-δ λ4102\n",
    "```\n",
    "\n",
    "\n",
    "FeII Lines, Table I in\n",
    "https://arxiv.org/pdf/1004.2212.pdf\n",
    "\n",
    "\n",
    "without FeII:\n",
    "\n",
    "lines = [3869,4102,4340,4686,4861,4959,5007,5876,6087,6300,6374,6563,6583,6678,6716,6731,7065]\n",
    "\n",
    "\n",
    "with Fe II:\n",
    "\n",
    "lines = [3869,4102,4340,4473,4489,4491,4508,4515,4520,4523,4534,4542,4549,4556,4576,4583,4621,4629,4667,4686,4731,4861,4924,4959,4993,5007,5018,5146,5169,5198,5235,5265,5276,5284,5317,5326,5338,5363,5414,5425,5876,6087,6300,6374,6563,6583,6678,6716,6731,7065]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read  data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-30T20:11:01.649369Z",
     "start_time": "2019-12-30T20:10:43.580051Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The feature size is 1x(7500-3800+1), a pixel to one wavelenght\n",
    "# merge data in previouly defined in different files\n",
    "type1 = read_data('archive-type1-z02-pixels-3800-7500-redshifted-lines-filtered-normalized.csv', delimiter=',')\n",
    "sy20 = read_data('archive-sy20-dr7-pixels-3800-7500-redshifted-lines-filtered-normalized-10000.csv', delimiter=',')\n",
    "sy19 = read_data('archive-sy19-dr7-pixels-3800-7500-redshifted-lines-filtered-normalized.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-30T20:11:01.653494Z",
     "start_time": "2019-12-30T20:11:01.651344Z"
    }
   },
   "outputs": [],
   "source": [
    "# spectrum length \n",
    "length = 2079"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-30T20:11:01.659219Z",
     "start_time": "2019-12-30T20:11:01.656961Z"
    }
   },
   "outputs": [],
   "source": [
    "## read sy19 with noise   \n",
    "#sy19_noise = []\n",
    "#for file in glob.glob('sy19-dr7-pixels-3800-7500-redshifted-lines-filtered-normalized-noise*csv'):\n",
    "#    print('reading:', file)\n",
    "#    file_feature = read_data(file, delimiter=',')\n",
    "#    sy19_noise = np.concatenate((sy19_noise, file_feature), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-30T20:11:02.023801Z",
     "start_time": "2019-12-30T20:11:01.662037Z"
    }
   },
   "outputs": [],
   "source": [
    "## Full sample\n",
    "## size of filtered spectrum 2079\n",
    "##size of original spectrum 3701\n",
    "\n",
    "type1_feature = torch.Tensor(np.concatenate((sy19, type1), axis=0)).view(-1,length)\n",
    "type2_feature = torch.Tensor(sy20).view(-1,length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-30T20:11:02.030419Z",
     "start_time": "2019-12-30T20:11:02.025270Z"
    }
   },
   "outputs": [],
   "source": [
    "# classify 1.9 as 1, and 2 as 0. \n",
    "type1_label = torch.ones(type1_feature.size(0),dtype=torch.long)\n",
    "type2_label = torch.zeros(type2_feature.size(0),dtype=torch.long)\n",
    "\n",
    "# create indices\n",
    "type1_indice = torch.linspace(0,len(type1_feature)-1,len(type1_feature)).int()\n",
    "type2_indice = torch.linspace(0,len(type2_feature)-1,len(type2_feature)).int()\n",
    "\n",
    "# taking same number of 1.9 and 2.0 seyfert galaxies\n",
    "type1_dataset = TensorDataset(type1_indice,type1_feature,type1_label)\n",
    "type2_dataset = TensorDataset(type2_indice[int(0.5*len(type1_feature)):int(1.5*len(type1_feature))],type2_feature[int(0.5*len(type1_feature)):int(1.5*len(type1_feature))],type2_label[int(0.5*len(type1_feature)):int(1.5*len(type1_feature))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-30T20:11:02.035201Z",
     "start_time": "2019-12-30T20:11:02.031676Z"
    }
   },
   "outputs": [],
   "source": [
    "# split to train and test datasets\n",
    "train_ratio = 0.75\n",
    "\n",
    "train_size = int(train_ratio * len(type1_dataset))\n",
    "test_size = len(type1_dataset) - train_size\n",
    "\n",
    "type1_dataset_train, type1_dataset_test = random_split(type1_dataset, [train_size, test_size])\n",
    "type2_dataset_train, type2_dataset_test = random_split(type2_dataset, [train_size, test_size])\n",
    "\n",
    "#type1_dataset_train = type1_dataset[:train_size]\n",
    "#type1_dataset_train = type1_dataset[train_size:]\n",
    "\n",
    "# merge the test dataset\n",
    "dataset = ConcatDataset((type1_dataset_train,type2_dataset_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Network - One layer of Full CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### computing output size of image\n",
    "\n",
    "\n",
    "- i = input\n",
    "- o = output\n",
    "- p = padding\n",
    "- k = kernel_size\n",
    "- s = stride\n",
    "- d = dilation\n",
    "\n",
    "o = [i + 2*p - k - (k-1)*(d-1)]/s + 1\n",
    "\n",
    "In your case this gives o = [32 + 2 - 3 - 2*1]/1 +1 = [29] + 1 = 30.\n",
    "Now, you could set all your parameters and “solve” the equation for p.\n",
    "You will see, that p=2 will give you an output size of 32.\n",
    "\n",
    "You could  with some tools like ezyang’s convolution visualizer 308 or calculate it with this formula:\n",
    "https://ezyang.github.io/convolution-visualizer/index.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Network Map\n",
    "\n",
    "| Layer | Type | Maps | Kernel_size | Padding | Stride | Image_size | Activation | Comment |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "|  Out | Linear | - | - | - | - | 2 | - |  |\n",
    "|  F7 | Linear | - | - | - | - | 100 | ReLU | 1000 -> 100  |\n",
    "|  D6 | Dropout (0.5) | - | - | - | - | - | - | - |\n",
    "|  C5 | Convolution | 1000 | 888 | 0 | 1 | 1 | ReLU |  |\n",
    "|  S4 | Avg Pooling | 128 | 2 | 0 | 2 | 888 | - |  |\n",
    "|  C3 | Convolution | 128 | 50 | 0 | 1 | 1777 | ReLU |  |\n",
    "|  S2 | Avg Pooling | 64 | 2 | 0 | 2 | 1826 | - |  |\n",
    "|  C1 | Convolution | 64 | 50 | 0 | 1 | 3652 | ReLU |  |\n",
    "| In | Input | 1 | - | - |- | 3701 |- | |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-30T20:11:02.042400Z",
     "start_time": "2019-12-30T20:11:02.036527Z"
    }
   },
   "outputs": [],
   "source": [
    "sizes = sizeMap(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-30T20:11:02.048244Z",
     "start_time": "2019-12-30T20:11:02.043588Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2079, 2030, 1015, 966, 483, 434, 217)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-30T20:11:02.211999Z",
     "start_time": "2019-12-30T20:11:02.049371Z"
    }
   },
   "outputs": [],
   "source": [
    "class simpleCNN(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(simpleCNN, self).__init__()\n",
    "        self.C1 = nn.Conv1d(1, 64, 50)\n",
    "        self.S2 = nn.AvgPool1d(2, stride=2)\n",
    "        self.C3 = nn.Conv1d(64, 128, 50)\n",
    "        self.S4 = nn.AvgPool1d(2, stride=2)\n",
    "        self.C5 = nn.Conv1d(128, 1000, sizes[4])\n",
    "        self.D6 = nn.Dropout(0.5)\n",
    "        self.F7 = nn.Linear(1000, 100)\n",
    "        self.Out= nn.Linear(100, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.C1(x),inplace=True)\n",
    "        x = self.S2(x)\n",
    "        x = F.relu(self.C3(x),inplace=True)\n",
    "        x = self.S4(x)\n",
    "        x = F.relu(self.C5(x),inplace=True)\n",
    "        x = self.D6(x)\n",
    "        x = x.view(x.size(0),-1)\n",
    "        x = F.relu(self.F7(x),inplace=True)\n",
    "        x = self.Out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-30T20:11:02.222059Z",
     "start_time": "2019-12-30T20:11:02.213603Z"
    }
   },
   "outputs": [],
   "source": [
    "class simplerCNN(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(simplerCNN, self).__init__()\n",
    "        self.C1 = nn.Conv1d(1, 64, 50)\n",
    "        self.B1 = nn.BatchNorm1d(64)\n",
    "        self.S2 = nn.AvgPool1d(2, stride=2)\n",
    "        self.C3 = nn.Conv1d(64, 128, 50)\n",
    "        self.B3 = nn.BatchNorm1d(128)\n",
    "        self.S4 = nn.AvgPool1d(2, stride=2)\n",
    "        self.C5 = nn.Conv1d(128, 128, 50)\n",
    "        self.B5 = nn.BatchNorm1d(128)\n",
    "        self.S6 = nn.AvgPool1d(2, stride=2)\n",
    "        self.D7 = nn.Dropout(0.5)\n",
    "        self.F8 = nn.Linear(sizes[6]*128, 1000)\n",
    "        self.F9 = nn.Linear(1000, 100)\n",
    "        self.Out= nn.Linear(100, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.B1(self.C1(x)),inplace=True)\n",
    "        x = self.S2(x)\n",
    "        x = F.leaky_relu(self.B3(self.C3(x)),inplace=True)\n",
    "        x = self.S4(x)\n",
    "        x = F.leaky_relu(self.B5(self.C5(x)),inplace=True)\n",
    "        x = self.S6(x)\n",
    "        x = self.D7(x)\n",
    "        x = x.view(x.size(0),-1)\n",
    "        x = F.relu(self.F8(x),inplace=True)\n",
    "        x = F.relu(self.F9(x),inplace=True)\n",
    "        x = self.Out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-30T20:11:02.230929Z",
     "start_time": "2019-12-30T20:11:02.223298Z"
    }
   },
   "outputs": [],
   "source": [
    "class simplestCNN(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(simplestCNN, self).__init__()\n",
    "        self.C1 = nn.Conv1d(1, 64, 50)\n",
    "        self.S2 = nn.AvgPool1d(2, stride=2)\n",
    "        self.C3 = nn.Conv1d(64, 128, 50)\n",
    "        self.S4 = nn.AvgPool1d(2, stride=2)\n",
    "        self.C5 = nn.Conv1d(128, 128, 50)\n",
    "        self.S6 = nn.AvgPool1d(2, stride=2)\n",
    "        self.D7 = nn.Dropout(0.5)\n",
    "        self.F8 = nn.Linear(sizes[6]*128, 1000)\n",
    "        self.F9 = nn.Linear(1000, 100)\n",
    "        self.Out= nn.Linear(100, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.C1(x),inplace=True)\n",
    "        x = self.S2(x)\n",
    "        x = F.relu(self.C3(x),inplace=True)\n",
    "        x = self.S4(x)\n",
    "        x = F.relu(self.C5(x),inplace=True)\n",
    "        x = self.S6(x)\n",
    "        x = self.D7(x)\n",
    "        x = x.view(x.size(0),-1)\n",
    "        x = F.relu(self.F8(x),inplace=True)\n",
    "        x = F.relu(self.F9(x),inplace=True)\n",
    "        x = self.Out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-30T20:11:02.515930Z",
     "start_time": "2019-12-30T20:11:02.232309Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simpleCNN(\n",
      "  (C1): Conv1d(1, 64, kernel_size=(50,), stride=(1,))\n",
      "  (S2): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
      "  (C3): Conv1d(64, 128, kernel_size=(50,), stride=(1,))\n",
      "  (S4): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
      "  (C5): Conv1d(128, 1000, kernel_size=(483,), stride=(1,))\n",
      "  (D6): Dropout(p=0.5, inplace=False)\n",
      "  (F7): Linear(in_features=1000, out_features=100, bias=True)\n",
      "  (Out): Linear(in_features=100, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = simpleCNN()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-30T20:11:02.696469Z",
     "start_time": "2019-12-30T20:11:02.517263Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0018, -0.0882],\n",
       "        [ 0.0249, -0.1057],\n",
       "        [ 0.0128, -0.1025],\n",
       "        [ 0.0022, -0.1203],\n",
       "        [-0.0019, -0.0915]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Test\n",
    "x = torch.randn(5,1,sizes[0])\n",
    "net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check GPU availabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-30T20:11:06.017701Z",
     "start_time": "2019-12-30T20:11:02.698111Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USE GPU\n"
     ]
    }
   ],
   "source": [
    "use_gpu = torch.cuda.is_available()\n",
    "if use_gpu:\n",
    "    net.cuda(0)\n",
    "    device = torch.device('cuda:0')\n",
    "    print ('USE GPU')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print ('USE CPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define  accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-30T20:11:06.024056Z",
     "start_time": "2019-12-30T20:11:06.019236Z"
    }
   },
   "outputs": [],
   "source": [
    "def accuracy(y_hat, y):\n",
    "#    y_hat = torch.sign(y_hat)\n",
    "    return (y_hat.argmax(dim=1) == y).float().mean().item()\n",
    "\n",
    "def evaluate_accuracy(data_iter, net, device):\n",
    "    acc = torch.tensor([0.0], device=device)\n",
    "    for (index, img, label) in data_iter:\n",
    "        img = img.view(-1,1,sizes[0])\n",
    "        # if device is GPU，copy to GPU\n",
    "        if use_gpu:\n",
    "            index, img, label = index.to(device), img.to(device), label.to(device)\n",
    "        acc += accuracy(net(img), label)\n",
    "    return acc.item() / len(data_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-30T20:11:06.029752Z",
     "start_time": "2019-12-30T20:11:06.027042Z"
    }
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "loss = nn.CrossEntropyLoss() # include softmax and cross entropy\n",
    "optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.99)\n",
    "#optimizer = optim.Adam(net.parameters(), lr=learning_rate, betas=(0.9,0.999), eps=1e-8)\n",
    "#optimizer = optim.SGD(net.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-12-30T20:13:09.289Z"
    }
   },
   "outputs": [],
   "source": [
    "num_epochs = 500\n",
    "\n",
    "batch_size = 64\n",
    "train_iter = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-12-30T20:13:09.291Z"
    }
   },
   "outputs": [],
   "source": [
    "## laod previous traning results\n",
    "#\n",
    "#net.load_state_dict(torch.load('cuda3-GOOD.pt'))\n",
    "#\n",
    "#previous = pd.read_csv('./runs/acc-list-cuda3-2019-07-22-15-02-10-826651-GOOD.csv', header=0, comment='#')\n",
    "#\n",
    "#loss_list = list(previous['loss'])\n",
    "#sy19_test_acc_list = list(previous['sy19_test_acc'])\n",
    "#sy20_test_acc_list = list(previous['sy20_test_acc'])\n",
    "#train_acc_list = list(previous['train_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-12-30T20:13:09.293Z"
    }
   },
   "outputs": [],
   "source": [
    "type1_test_acc_list = []\n",
    "type2_test_acc_list = []\n",
    "train_acc_list = []\n",
    "loss_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-12-30T20:13:09.295Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.677006, train acc 0.578776, type1 test acc  0.4720, type2 test acc  0.7171, time 20.9 sec\n",
      "epoch 2, loss 0.661693, train acc 0.616536, type1 test acc  0.8941, type2 test acc  0.3965, time 20.8 sec\n",
      "epoch 3, loss 0.669875, train acc 0.602911, type1 test acc  0.9256, type2 test acc  0.3052, time 20.9 sec\n",
      "epoch 4, loss 0.625716, train acc 0.650670, type1 test acc  0.6675, type2 test acc  0.7497, time 21.0 sec\n",
      "epoch 5, loss 0.557296, train acc 0.728213, type1 test acc  0.8962, type2 test acc  0.6123, time 21.0 sec\n",
      "epoch 6, loss 0.558173, train acc 0.726237, type1 test acc  0.6014, type2 test acc  0.8058, time 21.0 sec\n",
      "epoch 7, loss 0.636951, train acc 0.591588, type1 test acc  1.0000, type2 test acc  0.0000, time 21.1 sec\n",
      "epoch 8, loss 0.709296, train acc 0.489025, type1 test acc  0.0000, type2 test acc  1.0000, time 21.2 sec\n",
      "epoch 9, loss 0.689503, train acc 0.548689, type1 test acc  0.9951, type2 test acc  0.0186, time 21.2 sec\n",
      "epoch 10, loss 0.683748, train acc 0.543411, type1 test acc  0.0020, type2 test acc  1.0000, time 21.2 sec\n",
      "epoch 11, loss 0.695848, train acc 0.521903, type1 test acc  0.0000, type2 test acc  1.0000, time 21.2 sec\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    net.train() \n",
    "    start = time.time()\n",
    "    train_l_sum = 0.\n",
    "    train_acc_sum = 0.\n",
    "    for (index, img, label) in train_iter:\n",
    "        img = img.view(-1,1,sizes[0])\n",
    "        if use_gpu:\n",
    "            index, img, label = index.to(device), img.to(device), label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        predict = net(img)\n",
    "        l = loss(predict, label.view(-1))\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        train_l_sum += l.data.item()\n",
    "        train_acc_sum += accuracy(predict, label.view(-1))\n",
    "        #print(l)\n",
    "    \n",
    "    if ((epoch+1)%1 ==0):\n",
    "        net.eval() \n",
    "        type1_test_iter = DataLoader(type1_dataset_test, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "        type1_test_acc = evaluate_accuracy(type1_test_iter, net, device)\n",
    "        type2_test_iter = DataLoader(type2_dataset_test, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "        type2_test_acc = evaluate_accuracy(type2_test_iter, net, device)\n",
    "        \n",
    "        print('epoch %d, loss %.6f, train acc %.6f, type1 test acc  %.4f, type2 test acc  %.4f, '\n",
    "                'time %.1f sec' % (epoch + 1, train_l_sum / len(train_iter),\n",
    "                                    train_acc_sum / len(train_iter),\n",
    "                                    type1_test_acc, type2_test_acc, time.time() - start))\n",
    "        type1_test_acc_list.append(type1_test_acc)\n",
    "        type2_test_acc_list.append(type2_test_acc)\n",
    "        loss_list.append(train_l_sum / len(train_iter))\n",
    "        train_acc_list.append(train_acc_sum / len(train_iter))\n",
    "        if (train_l_sum / len(train_iter) < 0.1):\n",
    "            learning_rate = 0.001\n",
    "            optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)\n",
    "        if (train_l_sum / len(train_iter) < 0.02):\n",
    "            learning_rate = 0.0001\n",
    "            optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-12-30T20:13:09.297Z"
    }
   },
   "outputs": [],
   "source": [
    "## save results\n",
    "acc_lists = pd.DataFrame({'loss': loss_list, 'train_acc':train_acc_list, 'type1_test_acc': type1_test_acc_list, 'type2_test_acc': type2_test_acc_list})\n",
    "acc_lists_filename = 'acc-list-cuda0-'+str(datetime.datetime.now()).replace(' ','-').replace(':','-').replace('.','-')+'.csv'\n",
    "acc_lists.to_csv('./runs/'+acc_lists_filename)\n",
    "print(acc_lists_filename)\n",
    "\n",
    "model_filename = './runs/'+'model-cuda0-'+str(datetime.datetime.now()).replace(' ','-').replace(':','-').replace('.','-')+'.pt'\n",
    "torch.save(net.state_dict(), model_filename)\n",
    "print(model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-12-30T20:13:09.299Z"
    }
   },
   "outputs": [],
   "source": [
    "# save figure\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "epoches = np.linspace(1,len(loss_list),len(loss_list))\n",
    "plt.plot(epoches, train_acc_list, 's-', color='C0', linewidth=1, markersize=1.2,label='Train Accuracy: Total')\n",
    "plt.plot(epoches, (np.array(type1_test_acc_list)+np.array(type2_test_acc_list))/2, '.-',  color='C1', markersize=3.5, linewidth=1,label='Test Accuracy: Total')\n",
    "plt.plot(epoches, type1_test_acc_list, '2-', color='C2', linewidth=0,  markersize=4,label='Test Accuracy: Seyfert 1.9')\n",
    "plt.plot(epoches, type2_test_acc_list, '1-', color='C3', linewidth=0,  markersize=4,label='Test Accuracy: Seyfert 2.0')\n",
    "plt.plot(epoches, loss_list,'+-', color='C7', linewidth=0,  markersize=3,label='Train Loss')\n",
    "\n",
    "\n",
    "plt.axhline(y=1.0,color='grey',linestyle='dashed',linewidth=1, alpha=0.5, label='')\n",
    "#plt.axhline(y=0.84,color='C3',linestyle='solid',linewidth=3,  alpha=0.5,label='Accuracy: 83.0%')\n",
    "plt.axhline(y=0.94,color='C1',linestyle='solid',linewidth=1,  alpha=0.5,label='Accuracy: 94.0%')\n",
    "plt.axhline(y=0.81,color='C2',linestyle='solid',linewidth=3,  alpha=0.5,label='Accuracy: 81.0%')\n",
    "plt.axhline(y=0,color='grey',linestyle='dashed',linewidth=1,  alpha=0.5, label='')\n",
    "\n",
    "leg = plt.legend(title='Lines filtered', fontsize=10)\n",
    "leg._legend_box.align = \"center\"\n",
    "plt.setp(leg.get_title(),fontsize=10)\n",
    "plt.xlabel('Epoch',fontsize=12)\n",
    "plt.ylabel('Accuracy & Loss',fontsize=12)\n",
    "\n",
    "plt.xlim(0,100)\n",
    "plt.ylim(-0.02,1.05)\n",
    "\n",
    "figure_filename = './runs/'+'model-cuda3-'+str(datetime.datetime.now()).replace(' ','-').replace(':','-').replace('.','-')+'.pdf'\n",
    "fig.tight_layout()\n",
    "#plt.savefig(figure_filename, format='pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-12-30T20:13:09.301Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.semilogy(epoches, loss_list,label='loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
