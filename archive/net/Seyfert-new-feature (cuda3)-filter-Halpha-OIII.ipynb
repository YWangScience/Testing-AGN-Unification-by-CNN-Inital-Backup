{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traning for SY 1.9 & 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T03:24:30.183617Z",
     "start_time": "2019-11-18T03:24:29.461971Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import os\n",
    "import datetime\n",
    "import glob\n",
    "import csv\n",
    "\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, TensorDataset, ConcatDataset, random_split\n",
    "from torchvision import transforms\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from numpy import genfromtxt\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T03:24:32.111555Z",
     "start_time": "2019-11-18T03:24:32.105730Z"
    }
   },
   "outputs": [],
   "source": [
    "def plotSpec(seyfert,n):\n",
    "    if seyfert == 1.9:\n",
    "        plt.plot(np.linspace(3800,7500,7500-3800+1), sy19_feature[n].tolist())\n",
    "    if seyfert == 2:\n",
    "        plt.plot(np.linspace(3800,7500,7500-3800+1), sy2_feature[n].tolist())\n",
    "        \n",
    "def sizeMap(inputSize):\n",
    "    C1 = (inputSize-50)+1\n",
    "    S2 = int(C1/2)\n",
    "    C3 = (S2-50)+1\n",
    "    S4 = int(C3/2)\n",
    "    return C1, S2, C3, S4\n",
    "\n",
    "\n",
    "def read_data(filename, delimiter=','):\n",
    "    with open(filename, 'r') as my_file:\n",
    "        reader = csv.reader(my_file, delimiter=delimiter)\n",
    "        my_list = list(reader)[0]\n",
    "        my_list= [float(i) for i in my_list]\n",
    "        return my_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read  data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T03:24:57.878755Z",
     "start_time": "2019-11-18T03:24:36.467137Z"
    }
   },
   "outputs": [],
   "source": [
    "# The feature size is 1x(7500-3800+1), a pixel to one wavelenght\n",
    "# merge data in previouly defined in different files\n",
    "sy19_feature_train = read_data('sy19-pixels-3800-7500-redshifted-normalized.csv', delimiter=',')\n",
    "sy19_feature_test = read_data('sy19-test-pixels-3800-7500-redshifted-normalized.csv', delimiter=',')\n",
    "sy20_feature_1 = read_data('sy20-pixels-3800-7500-redshifted-normalized.csv', delimiter=',')\n",
    "sy20_feature_2 = read_data('sy20-test-pixels-3800-7500-redshifted-normalized.0.csv', delimiter=',')\n",
    "sy20_feature_3 = read_data('sy20-test-pixels-3800-7500-redshifted-normalized.1.csv', delimiter=',')\n",
    "sy20_feature_4 = read_data('sy20-test-pixels-3800-7500-redshifted-normalized.2.csv', delimiter=',')\n",
    "sy20_feature_5 = read_data('sy20-test-pixels-3800-7500-redshifted-normalized.3.csv', delimiter=',')\n",
    "sy20_feature_6 = read_data('sy20-test-pixels-3800-7500-redshifted-normalized.4.csv', delimiter=',')\n",
    "sy20_feature_7 = read_data('sy20-test-pixels-3800-7500-redshifted-normalized.5.csv', delimiter=',')\n",
    "sy20_feature_8 = read_data('sy20-test-pixels-3800-7500-redshifted-normalized.6.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T03:26:02.177405Z",
     "start_time": "2019-11-18T03:25:43.763357Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading: sy19-pixels-3800-7500-redshifted-normalized-noise.0.csv\n",
      "reading: sy19-pixels-3800-7500-redshifted-normalized-noise.1.csv\n",
      "reading: sy19-pixels-3800-7500-redshifted-normalized-noise.2.csv\n",
      "reading: sy19-pixels-3800-7500-redshifted-normalized-noise.3.csv\n",
      "reading: sy19-pixels-3800-7500-redshifted-normalized-noise.4.csv\n",
      "reading: sy19-pixels-3800-7500-redshifted-normalized-noise.5.csv\n",
      "reading: sy19-pixels-3800-7500-redshifted-normalized-noise.6.csv\n",
      "reading: sy19-pixels-3800-7500-redshifted-normalized-noise.7.csv\n",
      "reading: sy19-pixels-3800-7500-redshifted-normalized-noise.8.csv\n",
      "reading: sy19-pixels-3800-7500-redshifted-normalized-noise.9.csv\n",
      "reading: sy19-test-pixels-3800-7500-redshifted-normalized-noise.0.csv\n",
      "reading: sy19-test-pixels-3800-7500-redshifted-normalized-noise.1.csv\n",
      "reading: sy19-test-pixels-3800-7500-redshifted-normalized-noise.2.csv\n",
      "reading: sy19-test-pixels-3800-7500-redshifted-normalized-noise.3.csv\n",
      "reading: sy19-test-pixels-3800-7500-redshifted-normalized-noise.4.csv\n",
      "reading: sy19-test-pixels-3800-7500-redshifted-normalized-noise.5.csv\n",
      "reading: sy19-test-pixels-3800-7500-redshifted-normalized-noise.6.csv\n",
      "reading: sy19-test-pixels-3800-7500-redshifted-normalized-noise.7.csv\n",
      "reading: sy19-test-pixels-3800-7500-redshifted-normalized-noise.8.csv\n",
      "reading: sy19-test-pixels-3800-7500-redshifted-normalized-noise.9.csv\n"
     ]
    }
   ],
   "source": [
    "# read sy19 with noise   \n",
    "sy19_noise_feature_train = []\n",
    "for file in glob.glob('sy19-pixels*noise*'):\n",
    "    print('reading:', file)\n",
    "    file_feature = read_data(file, delimiter=',')\n",
    "    sy19_noise_feature_train = np.concatenate((sy19_noise_feature_train, file_feature), axis=0)\n",
    "\n",
    "sy19_noise_feature_test = []\n",
    "for file in glob.glob('sy19-test*noise*'):\n",
    "    print('reading:', file)\n",
    "    file_feature = read_data(file, delimiter=',')\n",
    "    sy19_noise_feature_test = np.concatenate((sy19_noise_feature_test, file_feature), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T03:26:25.951521Z",
     "start_time": "2019-11-18T03:26:25.167485Z"
    }
   },
   "outputs": [],
   "source": [
    "##  Small sample pre-traning\n",
    "#sy19_feature = sy19_feature_1\n",
    "#sy20_feature = sy20_feature_1\n",
    "\n",
    "## Full sample\n",
    "sy19_feature_train = np.concatenate((sy19_feature_train,  sy19_noise_feature_train), axis=0)\n",
    "#sy19_feature_test = np.concatenate((sy19_feature_test, sy19_noise_feature_test), axis=0)\n",
    "sy20_feature = np.concatenate((sy20_feature_1, sy20_feature_2,sy20_feature_3,sy20_feature_4,\\\n",
    "                               sy20_feature_5,sy20_feature_6,sy20_feature_7,sy20_feature_8), axis=0)\n",
    "\n",
    "sy19_feature_train = torch.Tensor(sy19_feature_train).view(-1,7500-3800+1)\n",
    "sy19_feature_test = torch.Tensor(sy19_feature_test).view(-1,7500-3800+1)\n",
    "sy20_feature = torch.Tensor(sy20_feature).view(-1,7500-3800+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T03:26:29.590939Z",
     "start_time": "2019-11-18T03:26:29.585492Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3300 445 8400\n"
     ]
    }
   ],
   "source": [
    "print(len(sy19_feature_train),len(sy19_feature_test),len(sy20_feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T03:26:51.973160Z",
     "start_time": "2019-11-18T03:26:51.931675Z"
    }
   },
   "outputs": [],
   "source": [
    "# filter wavelength\n",
    "## filter OIII 4959 and 5007, filter from 4600-5050, in total 450, \n",
    "## filter Halpha 6563, filter from 6400-6800, in total4300, \n",
    "\n",
    "## 4900-3800 = 1100\n",
    "## 5050-3800 = 1250\n",
    "## 6400-3800 = 2600\n",
    "## 6800-3800 = 3000\n",
    "sy19_feature_train = torch.cat([sy19_feature_train.narrow(1,0,1100),sy19_feature_train.narrow(1,1250,1350),\\\n",
    "                               sy19_feature_train.narrow(1,3000,701)],1)\n",
    "sy19_feature_test = torch.cat([sy19_feature_test.narrow(1,0,1100),sy19_feature_test.narrow(1,1250,1350),\\\n",
    "                               sy19_feature_test.narrow(1,3000,701)],1)\n",
    "sy20_feature = torch.cat([sy20_feature.narrow(1,0,1100),sy20_feature.narrow(1,1250,1350),\\\n",
    "                               sy20_feature.narrow(1,3000,701)],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T03:29:07.063011Z",
     "start_time": "2019-11-18T03:29:07.058677Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3151"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sy19_feature_train[1].size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T03:26:55.816053Z",
     "start_time": "2019-11-18T03:26:55.808284Z"
    }
   },
   "outputs": [],
   "source": [
    "# classify 1.9 as 1, and 2 as 0. \n",
    "sy19_label_train = torch.ones(sy19_feature_train.size(0),dtype=torch.long)\n",
    "sy19_label_test = torch.ones(sy19_feature_test.size(0),dtype=torch.long)\n",
    "sy20_label = torch.zeros(sy20_feature.size(0),dtype=torch.long)\n",
    "\n",
    "# create indices\n",
    "sy19_indice_train = torch.linspace(0,len(sy19_feature_train)-1,len(sy19_feature_train)).int()\n",
    "sy19_indice_test = torch.linspace(0,len(sy19_feature_test)-1,len(sy19_feature_test)).int()\n",
    "sy20_indice = torch.linspace(0,len(sy20_feature)-1,len(sy20_feature)).int()\n",
    "\n",
    "# taking same number of 1.9 and 2.0 seyfert galaxies\n",
    "sy19_dataset_train = TensorDataset(sy19_indice_train,sy19_feature_train,sy19_label_train)\n",
    "sy19_dataset_test = TensorDataset(sy19_indice_test,sy19_feature_test,sy19_label_test)\n",
    "\n",
    "sy20_dataset_train = TensorDataset(sy20_indice[:len(sy19_dataset_train)],sy20_feature[:len(sy19_dataset_train)],sy20_label[:len(sy19_dataset_train)])\n",
    "sy20_dataset_test = TensorDataset(sy20_indice[len(sy19_dataset_train):],sy20_feature[len(sy19_dataset_train):],sy20_label[len(sy19_dataset_train):])\n",
    "\n",
    "# merge the test dataset\n",
    "dataset = ConcatDataset((sy19_dataset_train,sy20_dataset_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-16T23:12:03.403326Z",
     "start_time": "2019-11-16T23:11:54.137Z"
    }
   },
   "outputs": [],
   "source": [
    "# split to train and test datasets\n",
    "#train_ratio = 0.75\n",
    "#train_size = int(train_ratio * len(sy19_dataset))\n",
    "#test_size = len(sy19_dataset) - train_size\n",
    "\n",
    "#sy19_train_dataset, sy19_test_dataset = random_split(sy19_dataset, [train_size, test_size])\n",
    "#sy20_train_dataset, sy20_test_dataset = random_split(sy20_dataset, [train_size, test_size])\n",
    "\n",
    "# merge the test dataset\n",
    "#dataset = ConcatDataset((sy19_train_dataset,sy20_train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-16T23:12:03.405396Z",
     "start_time": "2019-11-16T23:11:54.510Z"
    }
   },
   "outputs": [],
   "source": [
    "# merge data\n",
    "#features = torch.cat((sy19_feature,sy20_feature[:len(sy19_feature)]),0)\n",
    "#labels = torch.cat((sy19_label,sy20_label[:len(sy19_feature)]),0)\n",
    "#indices = torch.cat((sy19_indice,sy20_indice[:len(sy19_feature)]),0)\n",
    "\n",
    "## shuffle the dataset \n",
    "#dataset_rand = dataset[torch.randperm(len(dataset))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The feature size is 1x(7500-3800+1), a pixel to one wavelenght\n",
    "#sy19_test_feature = genfromtxt('sy19-test-pixels-3800-7500-redshifted-normalized.csv', delimiter=',')\n",
    "#sy20_test_feature = genfromtxt('sy20-test-pixels-3800-7500-redshifted-normalized.0.csv', delimiter=',')\n",
    "#\n",
    "#sy19_test_feature = torch.Tensor(sy19_test_feature).view(-1,7500-3800+1)\n",
    "#sy20_test_feature = torch.Tensor(sy20_test_feature).view(-1,7500-3800+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classify 1.9 as 1, and 2 as 0. \n",
    "#sy19_test_label = torch.ones(sy19_test_feature.size(0),dtype=torch.long)\n",
    "#sy20_test_label = torch.zeros(sy20_test_feature.size(0),dtype=torch.long)\n",
    "\n",
    "# create indices\n",
    "#sy19_test_indice = torch.linspace(0,len(sy19_test_feature)-1,len(sy19_test_feature)).int()\n",
    "#sy20_test_indice = torch.linspace(0,len(sy20_test_feature)-1,len(sy20_test_feature)).int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sy19_test_dataset = TensorDataset(sy19_test_indice,sy19_test_feature,sy19_test_label)\n",
    "#sy20_test_dataset = TensorDataset(sy20_test_indice,sy20_test_feature,sy20_test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Network - One layer of Full CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### computing output size of image\n",
    "\n",
    "\n",
    "- i = input\n",
    "- o = output\n",
    "- p = padding\n",
    "- k = kernel_size\n",
    "- s = stride\n",
    "- d = dilation\n",
    "\n",
    "o = [i + 2*p - k - (k-1)*(d-1)]/s + 1\n",
    "\n",
    "In your case this gives o = [32 + 2 - 3 - 2*1]/1 +1 = [29] + 1 = 30.\n",
    "Now, you could set all your parameters and “solve” the equation for p.\n",
    "You will see, that p=2 will give you an output size of 32.\n",
    "\n",
    "You could  with some tools like ezyang’s convolution visualizer 308 or calculate it with this formula:\n",
    "https://ezyang.github.io/convolution-visualizer/index.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Network Map\n",
    "\n",
    "| Layer | Type | Maps | Kernel_size | Padding | Stride | Image_size | Activation | Comment |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "|  Out | Linear | - | - | - | - | 2 | - |  |\n",
    "|  F7 | Linear | - | - | - | - | 100 | ReLU | 1000 -> 100  |\n",
    "|  D6 | Dropout (0.5) | - | - | - | - | - | - | - |\n",
    "|  C5 | Convolution | 1000 | 888 | 0 | 1 | 1 | ReLU |  |\n",
    "|  S4 | Avg Pooling | 128 | 2 | 0 | 2 | 888 | - |  |\n",
    "|  C3 | Convolution | 128 | 50 | 0 | 1 | 1777 | ReLU |  |\n",
    "|  S2 | Avg Pooling | 64 | 2 | 0 | 2 | 1826 | - |  |\n",
    "|  C1 | Convolution | 64 | 50 | 0 | 1 | 3652 | ReLU |  |\n",
    "| In | Input | 1 | - | - |- | 3701 |- | |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T03:29:17.622006Z",
     "start_time": "2019-11-18T03:29:17.619048Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3102, 1551, 1502, 751)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sizeMap(3151)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T03:29:25.643845Z",
     "start_time": "2019-11-18T03:29:25.628135Z"
    }
   },
   "outputs": [],
   "source": [
    "class simpleCNN(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(simpleCNN, self).__init__()\n",
    "        self.C1 = nn.Conv1d(1, 64, 50)\n",
    "        self.S2 = nn.AvgPool1d(2, stride=2)\n",
    "        self.C3 = nn.Conv1d(64, 128, 50)\n",
    "        self.S4 = nn.AvgPool1d(2, stride=2)\n",
    "        self.C5 = nn.Conv1d(128, 1000, 751)\n",
    "        self.D6 = nn.Dropout(0.5)\n",
    "        self.F7 = nn.Linear(1000, 100)\n",
    "        self.Out= nn.Linear(100, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.C1(x),inplace=True)\n",
    "        x = self.S2(x)\n",
    "        x = F.relu(self.C3(x),inplace=True)\n",
    "        x = self.S4(x)\n",
    "        x = F.relu(self.C5(x),inplace=True)\n",
    "        x = self.D6(x)\n",
    "        x = x.view(x.size(0),-1)\n",
    "        x = F.relu(self.F7(x),inplace=True)\n",
    "        x = self.Out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T03:29:30.456355Z",
     "start_time": "2019-11-18T03:29:30.018742Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simpleCNN(\n",
      "  (C1): Conv1d(1, 64, kernel_size=(50,), stride=(1,))\n",
      "  (S2): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
      "  (C3): Conv1d(64, 128, kernel_size=(50,), stride=(1,))\n",
      "  (S4): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
      "  (C5): Conv1d(128, 1000, kernel_size=(751,), stride=(1,))\n",
      "  (D6): Dropout(p=0.5, inplace=False)\n",
      "  (F7): Linear(in_features=1000, out_features=100, bias=True)\n",
      "  (Out): Linear(in_features=100, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = simpleCNN()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T03:29:38.734192Z",
     "start_time": "2019-11-18T03:29:38.457964Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0018,  0.0753],\n",
       "        [-0.0234,  0.0661],\n",
       "        [-0.0189,  0.0636],\n",
       "        [-0.0207,  0.0522],\n",
       "        [-0.0395,  0.0672]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Test\n",
    "x = torch.randn(5,1,3151)\n",
    "net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check GPU availabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T03:29:51.355548Z",
     "start_time": "2019-11-18T03:29:44.186571Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USE GPU\n"
     ]
    }
   ],
   "source": [
    "use_gpu = torch.cuda.is_available()\n",
    "if use_gpu:\n",
    "    net.cuda(3)\n",
    "    device = torch.device('cuda:3')\n",
    "    print ('USE GPU')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print ('USE CPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define optimizer, loss function and accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T03:29:56.888875Z",
     "start_time": "2019-11-18T03:29:56.886084Z"
    }
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "\n",
    "loss = nn.CrossEntropyLoss() # 包含了softmax 和 cross entropy\n",
    "optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)\n",
    "#optimizer = optim.Adam(net.parameters(), lr=learning_rate, betas=(0.9,0.999), eps=1e-8)\n",
    "#optimizer = optim.SGD(net.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T03:30:00.418480Z",
     "start_time": "2019-11-18T03:30:00.413843Z"
    }
   },
   "outputs": [],
   "source": [
    "def accuracy(y_hat, y):\n",
    "#    y_hat = torch.sign(y_hat)\n",
    "    return (y_hat.argmax(dim=1) == y).float().mean().item()\n",
    "\n",
    "def evaluate_accuracy(data_iter, net, device):\n",
    "    acc = torch.tensor([0.0], device=device)\n",
    "    for (index, img, label) in data_iter:\n",
    "        img = img.view(-1,1,3151)\n",
    "        # 如果 device 是 GPU，将数据复制到 GPU 上。\n",
    "        if use_gpu:\n",
    "            index, img, label = index.to(device), img.to(device), label.to(device)\n",
    "        acc += accuracy(net(img), label)\n",
    "    return acc.item() / len(data_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-16T23:34:17.247125Z",
     "start_time": "2019-11-16T23:34:17.245348Z"
    }
   },
   "outputs": [],
   "source": [
    "##original saved file with DataParallel\n",
    "#state_dict = torch.load('./cuda3.pt')\n",
    "#\n",
    "### create new OrderedDict that does not contain module.\n",
    "#from collections import OrderedDict\n",
    "#new_state_dict = OrderedDict()\n",
    "#\n",
    "#for k, v in state_dict.items():\n",
    "#    name = k[7:] # remove module.\n",
    "#    new_state_dict[name] = v\n",
    "#\n",
    "### load params\n",
    "#net.load_state_dict(new_state_dict)\n",
    "#\n",
    "#net.load_state_dict(torch.load('./cuda3.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T03:30:04.895385Z",
     "start_time": "2019-11-18T03:30:04.892927Z"
    }
   },
   "outputs": [],
   "source": [
    "num_epochs = 500\n",
    "\n",
    "batch_size = 64\n",
    "train_iter = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "## laod previous traning results\n",
    "#\n",
    "#net.load_state_dict(torch.load('cuda3-GOOD.pt'))\n",
    "#\n",
    "#previous = pd.read_csv('./runs/acc-list-cuda3-2019-07-22-15-02-10-826651-GOOD.csv', header=0, comment='#')\n",
    "#\n",
    "#loss_list = list(previous['loss'])\n",
    "#sy19_test_acc_list = list(previous['sy19_test_acc'])\n",
    "#sy20_test_acc_list = list(previous['sy20_test_acc'])\n",
    "#train_acc_list = list(previous['train_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T03:30:08.261244Z",
     "start_time": "2019-11-18T03:30:08.259015Z"
    }
   },
   "outputs": [],
   "source": [
    "sy19_test_acc_list = []\n",
    "sy20_test_acc_list = []\n",
    "train_acc_list = []\n",
    "loss_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-18T03:31:08.827Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.051633, train acc 0.980919, sy19 test acc  0.9082, sy20 test acc  0.9612, time 66.0 sec\n",
      "epoch 2, loss 0.045877, train acc 0.983173, sy19 test acc  0.8992, sy20 test acc  0.9649, time 66.2 sec\n",
      "epoch 3, loss 0.045472, train acc 0.982722, sy19 test acc  0.9193, sy20 test acc  0.9546, time 66.3 sec\n",
      "epoch 4, loss 0.040817, train acc 0.983774, sy19 test acc  0.9170, sy20 test acc  0.9587, time 66.3 sec\n",
      "epoch 5, loss 0.042607, train acc 0.982722, sy19 test acc  0.9170, sy20 test acc  0.9596, time 66.3 sec\n",
      "epoch 6, loss 0.040732, train acc 0.983624, sy19 test acc  0.8855, sy20 test acc  0.9658, time 66.4 sec\n",
      "epoch 7, loss 0.039818, train acc 0.985276, sy19 test acc  0.8803, sy20 test acc  0.9672, time 66.4 sec\n",
      "epoch 8, loss 0.037927, train acc 0.985276, sy19 test acc  0.9073, sy20 test acc  0.9592, time 66.3 sec\n",
      "epoch 9, loss 0.039557, train acc 0.984075, sy19 test acc  0.9120, sy20 test acc  0.9548, time 66.3 sec\n",
      "epoch 10, loss 0.035235, train acc 0.986478, sy19 test acc  0.9084, sy20 test acc  0.9603, time 66.3 sec\n",
      "epoch 11, loss 0.035181, train acc 0.986328, sy19 test acc  0.9170, sy20 test acc  0.9586, time 66.3 sec\n",
      "epoch 12, loss 0.035315, train acc 0.985577, sy19 test acc  0.9145, sy20 test acc  0.9570, time 66.3 sec\n",
      "epoch 13, loss 0.033091, train acc 0.985877, sy19 test acc  0.8788, sy20 test acc  0.9675, time 66.4 sec\n",
      "epoch 14, loss 0.032263, train acc 0.987230, sy19 test acc  0.8969, sy20 test acc  0.9626, time 66.3 sec\n",
      "epoch 15, loss 0.032507, train acc 0.987230, sy19 test acc  0.9145, sy20 test acc  0.9575, time 66.3 sec\n",
      "epoch 16, loss 0.030829, train acc 0.987530, sy19 test acc  0.8947, sy20 test acc  0.9643, time 66.4 sec\n",
      "epoch 17, loss 0.029855, train acc 0.987831, sy19 test acc  0.8875, sy20 test acc  0.9650, time 66.3 sec\n",
      "epoch 18, loss 0.032010, train acc 0.987530, sy19 test acc  0.9032, sy20 test acc  0.9621, time 66.3 sec\n",
      "epoch 19, loss 0.029699, train acc 0.989183, sy19 test acc  0.8713, sy20 test acc  0.9659, time 66.3 sec\n",
      "epoch 20, loss 0.028542, train acc 0.988882, sy19 test acc  0.9008, sy20 test acc  0.9601, time 66.2 sec\n",
      "epoch 21, loss 0.029235, train acc 0.988882, sy19 test acc  0.8715, sy20 test acc  0.9661, time 66.3 sec\n",
      "epoch 22, loss 0.030556, train acc 0.987680, sy19 test acc  0.8944, sy20 test acc  0.9633, time 66.3 sec\n",
      "epoch 43, loss 0.017555, train acc 0.994742, sy19 test acc  0.8950, sy20 test acc  0.9627, time 66.3 sec\n",
      "epoch 44, loss 0.015954, train acc 0.995042, sy19 test acc  0.8679, sy20 test acc  0.9680, time 66.3 sec\n",
      "epoch 45, loss 0.016104, train acc 0.995192, sy19 test acc  0.8918, sy20 test acc  0.9623, time 66.3 sec\n",
      "epoch 46, loss 0.015881, train acc 0.995192, sy19 test acc  0.8829, sy20 test acc  0.9657, time 66.3 sec\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    net.train() \n",
    "    start = time.time()\n",
    "    train_l_sum = 0.\n",
    "    train_acc_sum = 0.\n",
    "    for (index, img, label) in train_iter:\n",
    "        img = img.view(-1,1,3151)\n",
    "        if use_gpu:\n",
    "            index, img, label = index.to(device), img.to(device), label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        predict = net(img)\n",
    "        l = loss(predict, label.view(-1))\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        train_l_sum += l.data.item()\n",
    "        train_acc_sum += accuracy(predict, label.view(-1))\n",
    "        #print(l)\n",
    "    \n",
    "    if ((epoch+1)%1 ==0):\n",
    "        net.eval() \n",
    "        sy19_test_iter = DataLoader(sy19_dataset_test, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "        sy19_test_acc = evaluate_accuracy(sy19_test_iter, net, device)\n",
    "        sy20_test_iter = DataLoader(sy20_dataset_test, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "        sy20_test_acc = evaluate_accuracy(sy20_test_iter, net, device)\n",
    "        \n",
    "        print('epoch %d, loss %.6f, train acc %.6f, sy19 test acc  %.4f, sy20 test acc  %.4f, '\n",
    "                'time %.1f sec' % (epoch + 1, train_l_sum / len(train_iter),\n",
    "                                    train_acc_sum / len(train_iter),\n",
    "                                    sy19_test_acc, sy20_test_acc, time.time() - start))\n",
    "        sy19_test_acc_list.append(sy19_test_acc)\n",
    "        sy20_test_acc_list.append(sy20_test_acc)\n",
    "        loss_list.append(train_l_sum / len(train_iter))\n",
    "        train_acc_list.append(train_acc_sum / len(train_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-18T03:31:11.131Z"
    }
   },
   "outputs": [],
   "source": [
    "## save results\n",
    "acc_lists = pd.DataFrame({'loss': loss_list, 'train_acc':train_acc_list, 'sy19_test_acc': sy19_test_acc_list, 'sy20_test_acc': sy20_test_acc_list})\n",
    "acc_lists_filename = 'acc-list-cuda0-'+str(datetime.datetime.now()).replace(' ','-').replace(':','-').replace('.','-')+'.csv'\n",
    "acc_lists.to_csv('./runs/'+acc_lists_filename)\n",
    "print(acc_lists_filename)\n",
    "\n",
    "model_filename = './runs/'+'model-cuda0-'+str(datetime.datetime.now()).replace(' ','-').replace(':','-').replace('.','-')+'.pt'\n",
    "torch.save(net.state_dict(), 'model_filename')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-17T15:09:27.416861Z",
     "start_time": "2019-11-17T15:09:26.871715Z"
    }
   },
   "outputs": [],
   "source": [
    "epoches = np.linspace(1,len(loss_list),len(loss_list))\n",
    "plt.plot(epoches, train_acc_list, '-', linewidth=1,label='Train Accuracy: Total')\n",
    "plt.plot(epoches, sy19_test_acc_list, '-', linewidth=1,label='Test Accuracy: Seyfert 1.9')\n",
    "plt.plot(epoches, sy20_test_acc_list, '-', linewidth=1,label='Test Accuracy: Seyfert 2.0')\n",
    "plt.plot(epoches, loss_list,'-', linewidth=1,label='Train Loss')\n",
    "\n",
    "\n",
    "plt.axhline(y=1.0,color='grey',linestyle='solid',linewidth=0.5, alpha=0.5, label='')\n",
    "plt.axhline(y=0.965,color='green',linestyle='dashed',linewidth=0.5,  label='Accuracy: 96.5%')\n",
    "plt.axhline(y=0.89,color='orange',linestyle='dashed',linewidth=0.5,  label='Accuracy: 89%')\n",
    "plt.axhline(y=0,color='grey',linestyle='solid',linewidth=0.5,  alpha=0.5, label='')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy & Loss')\n",
    "\n",
    "plt.xlim(-10,280)\n",
    "plt.ylim(-0.02,1.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
